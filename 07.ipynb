{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7CUSMSDA Practical Week 7 :  Spatial Autocorrelation & Spatial Regression\n",
    "<a href=\"#This Week's Overview\">This Week's Overview</a>\n",
    "\n",
    "<a href=\"#Learn Outcomes\">Learn Outcomes</a> \n",
    "\n",
    "<a href='#Get prepared'>Get prepared</a>\n",
    "\n",
    "<a href='#Spatial Weights'>Spatial Weights</a>\n",
    "  - <a href='#Contiguity Based Weights'>Contiguity Based Weights<a/>\n",
    "  - <a href='#Distance Based Weights'>Distance Based Weights</a>\n",
    "  - <a href='#Kernel Weights'>Kernel Weights<a/>\n",
    "\n",
    "<a href='#Spatial Lag'>Spatial Lag</a>\n",
    "  - <a href='#Spatial Similarity'>Spatial Similarity<a/>\n",
    "  - <a href='#Moran Plot'>Moran Plot<a/>\n",
    "  - <a href='#Global spatial autocorrelation'>Global spatial autocorrelation<a/>\n",
    "  - <a href='#Local spatial autocorrelation'>Local spatial autocorrelation<a/>\n",
    "    \n",
    "<a href='#Spatial Regression'>Spatial Regression</a>\n",
    "  - <a href='#Spatial Lag model'>Spatial Lag model<a/>\n",
    "  - <a href='#Spatial Error model'>Spatial Error model<a/>\n",
    "  - <a href='#Prediction performance of spatial models'>Prediction performance of spatial models</a>\n",
    "  - <a href='#GWR Prediction'>GWR Prediction<a/>\n",
    "\n",
    "- <a href='#Task 1'>Task 1<a/>\n",
    "- <a href='#Task 2'>Task 2<a/>\n",
    "- <a href='#Task 3'>Task 3<a/>\n",
    "- <a href='#Task 4'>Task 4<a/>\n",
    "- <a href='#Task 5'>Task 5<a/>\n",
    "- <a href='#Task 6 (Optional)'>Task 6 (Optional)<a/>\n",
    "- <a href='#Task 7'>Task 7<a/>\n",
    "- <a href='#Task 8'>Task 8<a/>\n",
    "- <a href='#Task 9'>Task 9<a/>\n",
    "- <a href='#Task 10'>Task 10<a/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"This Week's Overview\">This Week's Overview</a>\n",
    "This practical will make you more confident with your understanding of Spatial Weights by 3 main types: the widely used contiguity based weights, the distance based weights and kernel weights. You will be provided the functions from `PySAL` and `Libpysal` to explore the features for corresponding functions, and conduct further comparisons among the results. Upon the interpretation of spatial weights, concepts of `Spatial Lag` and `Global Spatial Autocorrelation` will be presented with variables on london housing, as well as detailed explanations on their processes and corresponding visualizations.\n",
    "\n",
    "Since Moran's I value can only tells us the existence of global spatial autocorrelation, but incapable to help identifying where the clusters are; in another word, if we want explore further on the spatial instability incurred by particular areas' departuring from the general pattern, we need to explore some local measures to obtain further insight by using `Local Indicators of Spatial Association (LISAs)` in `PySAL` to classify observations in our dataset into four groups, each of which are based on Moran plot and called \"quadrants\".\n",
    "- high values surrounded by high values (HH), in what we call `hot spots`.\n",
    "- low values nearby other low values (LL), in what we call `cold spots`.\n",
    "- high values among low values (HL), in what we call `spatial outliers`.\n",
    "- low values among high values (LH), in what we call `spatial outliers`. \n",
    "\n",
    "Spatial regression models will be introduced with both Spatial Lag model and Spatial Error model for your comparison, and GWR (geographical weighted regression) prediction will also follow as the end of this practical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"Learn Outcomes\">Learn Outcomes</a>\n",
    "You will practice your understanding on the concepts delivered in lecture, which are:\n",
    "- Spatial weights (Contiguity based, Distance based, kernel weights)\n",
    "- Spatial Autocorrelation (Global & Local)\n",
    "- Spatial Regression models (Lag & Error)\n",
    "- GWR prediction\n",
    "\n",
    "You will further explore the functions provided in `PySAL` and `Libpysal`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"Get prepared\">Get prepared</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import libpysal as lps\n",
    "from libpysal.weights import KNN\n",
    "import pysal as ps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pysal.viz as viz\n",
    "\n",
    "from pysal.model.spreg import ols\n",
    "from pysal.model.spreg import ML_Error\n",
    "from pysal.model.spreg import ML_Lag\n",
    "\n",
    "# GWR prediction libraries\n",
    "from pysal.model.mgwr.sel_bw import Sel_BW\n",
    "from pysal.model.mgwr.gwr import GWR\n",
    "# from pysal.contrib.glm.family import Gaussian\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will glue to only 1 set of data, so please copy the **shapefile data \"borough_airbnb_housing\"** we produced last week into your \"data\" folder for this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        Code                       Area                  Year      Value  \\\n0  E01000001        City of London 001A  Year ending Dec 2017  1204928.0   \n1  E01000002        City of London 001B  Year ending Dec 2017   991549.0   \n2  E01000003        City of London 001C  Year ending Dec 2017   913007.0   \n3  E01000006  Barking and Dagenham 016A  Year ending Dec 2017   354300.0   \n4  E01000007  Barking and Dagenham 015A  Year ending Dec 2017   230380.0   \n\n  Measure  objectid                  lsoa11nmw     st_areasha   st_lengths  \\\n0    Mean         1        City of London 001A  133320.768872  2291.846072   \n1    Mean         2        City of London 001B  226191.272990  2433.960112   \n2    Mean         3        City of London 001C   57302.966538  1142.359799   \n3    Mean         5  Barking and Dagenham 016A  144195.846857  1935.510354   \n4    Mean         6  Barking and Dagenham 015A  198134.809244  2824.036914   \n\n   IMD_Rand  ...  TotDec DepChi Pop16_59         Pop60_      WorkPop  \\\n0     29199  ...     656    465   715.00  343907.419830  3682.439420   \n1     30379  ...     580    394   619.75  583474.041779  3910.387240   \n2     14915  ...     759    445   804.00  147839.506081  1834.931320   \n3     14486  ...    1297    221  1284.50  372257.321186  3108.610781   \n4      7256  ...    1424    105  1404.00  511543.283051  4537.675635   \n\n   Mean Price  Small Host  Multiple L  Property C  \\\n0  148.444444         8.0        10.0        18.0   \n1  200.400000         8.0         2.0        10.0   \n2  139.428571         5.0         2.0         7.0   \n3   44.200000         5.0         0.0         5.0   \n4   62.000000        10.0         4.0        14.0   \n\n                                            geometry  \n0  POLYGON ((532095.563 181577.351, 532095.125 18...  \n1  POLYGON ((532267.728 181643.781, 532262.875 18...  \n2  POLYGON ((532105.312 182010.574, 532104.872 18...  \n3  POLYGON ((544817.826 184346.261, 544815.791 18...  \n4  POLYGON ((544175.331 184526.180, 544175.880 18...  \n\n[5 rows x 74 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Code</th>\n      <th>Area</th>\n      <th>Year</th>\n      <th>Value</th>\n      <th>Measure</th>\n      <th>objectid</th>\n      <th>lsoa11nmw</th>\n      <th>st_areasha</th>\n      <th>st_lengths</th>\n      <th>IMD_Rand</th>\n      <th>...</th>\n      <th>TotDec</th>\n      <th>DepChi</th>\n      <th>Pop16_59</th>\n      <th>Pop60_</th>\n      <th>WorkPop</th>\n      <th>Mean Price</th>\n      <th>Small Host</th>\n      <th>Multiple L</th>\n      <th>Property C</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>E01000001</td>\n      <td>City of London 001A</td>\n      <td>Year ending Dec 2017</td>\n      <td>1204928.0</td>\n      <td>Mean</td>\n      <td>1</td>\n      <td>City of London 001A</td>\n      <td>133320.768872</td>\n      <td>2291.846072</td>\n      <td>29199</td>\n      <td>...</td>\n      <td>656</td>\n      <td>465</td>\n      <td>715.00</td>\n      <td>343907.419830</td>\n      <td>3682.439420</td>\n      <td>148.444444</td>\n      <td>8.0</td>\n      <td>10.0</td>\n      <td>18.0</td>\n      <td>POLYGON ((532095.563 181577.351, 532095.125 18...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>E01000002</td>\n      <td>City of London 001B</td>\n      <td>Year ending Dec 2017</td>\n      <td>991549.0</td>\n      <td>Mean</td>\n      <td>2</td>\n      <td>City of London 001B</td>\n      <td>226191.272990</td>\n      <td>2433.960112</td>\n      <td>30379</td>\n      <td>...</td>\n      <td>580</td>\n      <td>394</td>\n      <td>619.75</td>\n      <td>583474.041779</td>\n      <td>3910.387240</td>\n      <td>200.400000</td>\n      <td>8.0</td>\n      <td>2.0</td>\n      <td>10.0</td>\n      <td>POLYGON ((532267.728 181643.781, 532262.875 18...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>E01000003</td>\n      <td>City of London 001C</td>\n      <td>Year ending Dec 2017</td>\n      <td>913007.0</td>\n      <td>Mean</td>\n      <td>3</td>\n      <td>City of London 001C</td>\n      <td>57302.966538</td>\n      <td>1142.359799</td>\n      <td>14915</td>\n      <td>...</td>\n      <td>759</td>\n      <td>445</td>\n      <td>804.00</td>\n      <td>147839.506081</td>\n      <td>1834.931320</td>\n      <td>139.428571</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>7.0</td>\n      <td>POLYGON ((532105.312 182010.574, 532104.872 18...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>E01000006</td>\n      <td>Barking and Dagenham 016A</td>\n      <td>Year ending Dec 2017</td>\n      <td>354300.0</td>\n      <td>Mean</td>\n      <td>5</td>\n      <td>Barking and Dagenham 016A</td>\n      <td>144195.846857</td>\n      <td>1935.510354</td>\n      <td>14486</td>\n      <td>...</td>\n      <td>1297</td>\n      <td>221</td>\n      <td>1284.50</td>\n      <td>372257.321186</td>\n      <td>3108.610781</td>\n      <td>44.200000</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>POLYGON ((544817.826 184346.261, 544815.791 18...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>E01000007</td>\n      <td>Barking and Dagenham 015A</td>\n      <td>Year ending Dec 2017</td>\n      <td>230380.0</td>\n      <td>Mean</td>\n      <td>6</td>\n      <td>Barking and Dagenham 015A</td>\n      <td>198134.809244</td>\n      <td>2824.036914</td>\n      <td>7256</td>\n      <td>...</td>\n      <td>1424</td>\n      <td>105</td>\n      <td>1404.00</td>\n      <td>511543.283051</td>\n      <td>4537.675635</td>\n      <td>62.000000</td>\n      <td>10.0</td>\n      <td>4.0</td>\n      <td>14.0</td>\n      <td>POLYGON ((544175.331 184526.180, 544175.880 18...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows √ó 74 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# read in your data and get the headers presented\n",
    "gdf=gpd.read_file('data/lsoa_IMD_airbnb_housing.shp')\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Task 1'>Task 1<a/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to get a rough idea about the distribution frequency of airbnbs across lsoas in London, we can plot a quantile map. So recall your memory to draw a simple quantile map for $variable$ 'Property C' and set up the $cmap$ value as 'coolwarm'.\n",
    "\n",
    "**Hint**: scheme to be 'quantiles'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "gdf.plot(column='Property C', alpha=0.8, cmap='coolwarm', scheme='quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as we are going to calculate the spatial weights by distance today, we need to be more careful about **crs**; it means that we need to reset the crs to epsg 27700 again, but I am sure you are more familiar with the steps now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(12,10))\n",
    "# Plot Number of Airbnbs\n",
    "# Quickly transform to OSGB CRS and plot \n",
    "gdf.plot(column='Property C', scheme='Quantiles', legend=True, ax=ax)\n",
    "# Remove axis frame\n",
    "ax.set_axis_off()\n",
    "# Change background color of the figure\n",
    "f.set_facecolor('0.8')\n",
    "# set up the title\n",
    "f.suptitle('Amount of Airbnbs in LSOAs', size=25)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar maps but stretched, right? Once you are prepared, let's start our spatial calculation.\n",
    "## <a id='Spatial Weights'>Spatial Weights</a>\n",
    "\n",
    "Spatial weights are mathematical structures representing for spatial relationships, and are crucial components of spatial analysis. Generally it is a $n$x$n$ matrix measuring the potential spatial relationships between paired observations in a spatial datasetÔºåwhich is on $n$ locations with varied geometries. The spatial relationships between these geometries can be based on criteria like:\n",
    "- Contiguity Based Weights\n",
    "- Distance Based Weights (both geospatial distance and general distance)\n",
    "- Kernel Weights\n",
    "\n",
    "In spatial weights matrix, the geographical space is encoded into numerical form for statistical practice.The elements in diagons $w_{ii}$ are set to zero while the rest cells $w_{ij}$ measure the potential interactions between each pair at location $i$ and $j$.\n",
    "\n",
    "We are going to realize the function using `PySAL` to create, manipulate and analyze spatial weights matrices across different types in the following section. For further details see the Spatial Weights [API](https://pysal.readthedocs.io/en/latest/api.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Contiguity Based Weights'>Contiguity Based Weights</a>\n",
    "\n",
    "Contiguity Weights can be built from dataframe with a geometry column or from  contiguity graph representation, e.g.shapefile. In this section, we will use contiguity to define neighboring observations: use `weights.Contiguity` module in `PySAL` to constructe and manipulate spatial weights matrices based on contiguity criteria; and to use `weights.Contiguity` in `libpysal` to further get the idea plotted out.\n",
    "\n",
    "Three contiguity weights will be compared: **Queen**, **Rook** and **Bishop**.\n",
    "#### Queen contiguity weight\n",
    "This commonly used weight type build a queen contiguity matrix for our data, , reflecting the adjacency relationship whether a polygon shares an **edge** or a **vertex** with another polygon or not. A pair of boroughs to be considered neighbours under this $W$ will need to \"touch\" each other to some degree. As the weights are symmetric, if borough $A$ neighbors borough $B$, then both $w_{AB} = 1$ and $w_{BA} = 1$.\n",
    "\n",
    "We will begin with the `GeoDataFrame` and pass it on to the queen contiguity weights builder in `PySAL` (`ps.lib.weights.Queen.from_dataframe`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4201"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Create the spatial weights matrix\n",
    "w_queen = ps.lib.weights.Queen.from_dataframe(gdf)\n",
    "w_queen.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('%.4f'%w_queen.pct_nonzero) # percentage of non zero queen weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_queen.histogram # frequency of n neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.loc[gdf.Area=='Westminster 018A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index used in weights are the same with dataframe, so let's try to check which LSOAs are neighbors of observation `Westminster 018A` with index `4000`, and how much they are \"weighted\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_queen[4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you get the name list for neighbours? For example, put the the target lsoa and its neighbours' indexes and names presented in one list. You should get a list including Westminster and Camden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lsoa = [4000]\n",
    "target_lsoa.extend(w_queen.neighbors[4000])\n",
    "gdf.loc[target_lsoa]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us row-standardize it to make sure every row of the matrix sums up to one, and check the neighbours for Westminster 018A again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row standardize the matrix\n",
    "w_queen.transform = 'R'\n",
    "w_queen[4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you get this time? The weight given to each neighbour has changed from 1.0 to 0.2! Think of the reason, and the sum of their weights, it should be 1 after the row-standardizing.\n",
    "We call `pysal.full` to get a full, dense matrix describing all of the pairwise relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wqmatrix, ids = w_queen.full()\n",
    "wqmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = wqmatrix.sum(axis=1) # how many neighbors each region has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors[4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now get a direct image of the Queen adjacent regions/neighbors for Westminster 018A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "queen_neighs=w_queen.neighbors[4000]\n",
    "q=gdf.loc[queen_neighs].plot(edgecolor='grey', facecolor='w')\n",
    "title=plt.title('Queen Adjacency for Westminster 018A')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, to visualize the Queen contiguity weights from neighbors, we need to call `libpysal` further, which is more visiable of the contiguity when plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_queen_1 = lps.weights.Queen.from_dataframe(gdf)\n",
    "type(w_queen_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you may find the returned type of the same weight matrix is different! Because you've called another library `libpysal`. Let's have a look of their contiguity by calling `plot`. \n",
    "\n",
    "**!! Be patient**, it takes time to get the plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = gdf.plot(edgecolor='grey', facecolor='w', figsize=(20, 12))\n",
    "f,ax = w_queen_1.plot(gdf, ax=ax, \n",
    "                   edge_kws=dict(color='r', linestyle=':', linewidth=1),\n",
    "                   node_kws=dict(marker=''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of the rationale for using queen weights from libpysal, rather than that from pysal, for plotting. How if you change it to the latter? What you may get and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rook contiguity weights\n",
    "\n",
    "Rook weights define neighbors as those sharing an edge on their respective borders. At finer scales, the rook neighbors of an observation may be different from the queen neighbors, depending on the configuration of both targeted observation and 'neighbors'. This time we use `.from_shapefile` function to get the rook neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rook = ps.lib.weights.Rook.from_shapefile('data/lsoa_IMD_airbnb_housing.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Task 2'>Task 2<a/>\n",
    "Since we get a new spatial weight matrix by using Rook rather than Queen, we need do similar work to get the corresponding values as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of rows or columns\n",
    "w_rook.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of nonzero neighbor counts\n",
    "w_rook.pct_nonzero  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram\n",
    "w_rook.histogram  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices for neighbors of Westminster 018A indexed at 4000\n",
    "w_rook.neighbors[4000]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the neighboring lsoa names for Westminster 018A indexed at 4000\n",
    "gdf['Area'][w_rook.neighbors[4000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Westminster 018A has 5 rook neighbor LSOAs in Westminster and Camden; the same as queen neighbors at lsoa level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# plot Westminster 018A and 5 rook adjacent neighbors\n",
    "# your code here\n",
    "rook_neighs=w_rook.neighbors[4000]\n",
    "r=gdf.loc[rook_neighs].plot(edgecolor='grey', facecolor='w')\n",
    "title=plt.title('Rook Adjacency')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Task 3'>Task 3<a/>\n",
    "Similarly, we can try to call `libpysal` to get the spatial weights visualized.\n",
    "    \n",
    "**Be patient again!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rook_1 = lps.weights.Rook.from_shapefile('data/lsoa_IMD_airbnb_housing.shp')\n",
    "type(w_rook_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# plot out the contiguity \n",
    "ax = gdf.plot(edgecolor='grey', facecolor='w', figsize=(20, 12))\n",
    "f,ax = w_rook_1.plot(gdf, ax=ax, \n",
    "                   edge_kws=dict(color='r', linestyle=':', linewidth=1),\n",
    "                   node_kws=dict(marker=''))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove our test on the similarity of results between Queen weight and Rook weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " (w_queen.pct_nonzero == w_rook.pct_nonzero) and (w_queen.n == w_rook.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bishop contiguity weights\n",
    "\n",
    "Bishop weighting only consider polygons as neighbors when they share vertexes. It is not directly available from `PySAL`, but we can construct it by using `w_difference` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_bishop = ps.lib.weights.w_difference(w_queen, w_rook, constrained=False, silence_warnings=True)\n",
    "w_bishop.histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram result, we can tell for this dataset: LSOA Westminster 018A has some bishop neighbors, which means these lsoas only share vertexes without sharing any edges; we can use this function or simply call the `islands` to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "print (w_bishop.islands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Task 4 (Optional)'>Task 4 (Optional)<a/>\n",
    "Get both Rook weights and Queen weights plotted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# plot them, your code here\n",
    "f,ax = plt.subplots(1,2,figsize=(10, 6), subplot_kw=dict(aspect='equal'))\n",
    "# plot the rook, set the title and axis \n",
    "w_rook_1.plot(gdf, ax=ax[0], edge_kws=dict(color='b', linestyle='-', linewidth=1), node_kws=dict(marker=''))\n",
    "w_queen_1.plot(gdf, ax=ax[1], \n",
    "                   edge_kws=dict(color='r', linestyle=':', linewidth=1),\n",
    "                   node_kws=dict(marker=''))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='Distance Based Weights'>Distance Based Weights</a>\n",
    "Besides of contiguity defined neighbors, we can also use distance to define neighbors in a more common way. We can use [`weights.Distance` module](https://pysal.readthedocs.io/en/latest/library/weights/Distance.html) in `PySAL`. However, if you recap on what we've done on Week 4 and Week 5, the measurement of distance should be careful about crs. So we need ensure the shapefile used has been projected in the right way.\n",
    "#### k-nearest neighbor weights\n",
    "We use k-nearest neighbor criterion to define neighbors for target observation. For example, we set $k=4$ for a trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_knn = ps.lib.weights.KNN.from_dataframe(gdf, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_knn.histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_knn.s0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use this function to call all the neighbors' list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "listnei = w_knn.reweight(p=1, inplace=False)\n",
    "print (listnei.neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libpysal.weights import KNN\n",
    "w_knn_1 = KNN.from_dataframe(gdf, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get it plotted again\n",
    "ax = gdf.plot(edgecolor='grey', facecolor='w')\n",
    "f,ax = w_knn_1.plot(gdf, ax=ax, \n",
    "        edge_kws=dict(color='r', linestyle=':', linewidth=1),\n",
    "        node_kws=dict(marker=''))\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $k$ value could be adjusted when we want to change the weights by calling `reweight`, this will help us to change the weight object without re-constructing the KDTree when computing the nearest neighbors queries. For example, we want to change the $k$ value into 5: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_knn_1.reweight(k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Task 5'>Task 5<a/>\n",
    "Let's still take Westminster 018A as an example again, and compare the outputs for K=5 with k=4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the w_knn neighbors list for Westminster 018A\n",
    "# hint: simply call its index\n",
    "w_knn[4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the reweight function to reset the k as 5\n",
    "w_knn_r = w_knn.reweight(k=5, inplace=False)\n",
    "# new list of neighbors for Westminster 018A\n",
    "w_knn_r[4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new s0 value\n",
    "w_knn_r.s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(w_knn_r.neighbors[4000]) == set([744, 4001, 4153, 743, 4024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_knn_r.weights[4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Band Weights (Optional)\n",
    "You may observed already that using $knn$ weights, we will get all target observations with same number of neighbors. If we use  distance bands or thresholds to define neighbors, to find those falling into the defined threshold distance, then the neighbors vary. The distance band weights could be generated from array, dataframe, shapefile, specified values, etc. We are here to try get an example from dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_thresh=ps.lib.weights.DistanceBand.from_dataframe(gdf, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_thresh.weights[4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Kernel Weights'>Kernel Weights<a/> <font color='red'>Don't need to run the code, your laptop will crash!</font> \n",
    "\n",
    "We had used Kernels for several times, and this kernel weights combine the aforementioned thresholds and continuously numeric weights together, to define neighbors by continuous distance-based weights using kernel densities. Upon the defined bandwidth, a continuous kernel function is evaluated to get a weight between 0 and 1, hence many kernels could be called on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_kernel = ps.lib.weights.Kernel.from_dataframe(gdf)\n",
    "w_kernel.neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kw_1](kw_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_kernel.weights[4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kw_2](kw_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.loc[w_kernel.neighbors[4000] + [4000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kw_3](kw_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_kernel.bandwidth[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kw_4](kw_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling nonplanar geometries, and try to compare it with the output of Queen weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fuzzy_non = lps.weights.fuzzy_contiguity(gdf)\n",
    "len(w_fuzzy_non.islands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = gdf.plot(edgecolor='grey', facecolor='w')\n",
    "f,ax = w_fuzzy_non.plot(gdf, ax=ax, \n",
    "        edge_kws=dict(color='r', linestyle=':', linewidth=1),\n",
    "        node_kws=dict(marker=''))\n",
    "ax.set_title('Nonplanar Weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kw_5](kw_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='Spatial Lag'>Spatial Lag<a/>\n",
    "`Spatial lag` is the product of the spatial weights matrix and a given variable and that, if ùëä is row-standardized, the result amounts to the average value of the variable in the neighborhood of each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use variable for average airbnb listing price (`Mean Price`) as an example to interpret the concept of `spatial lag`. Firstly let's have a general idea of the spatial distribution pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = ps.viz.mapclassify.Quantiles(gdf['Mean Price'], k=5)\n",
    "f, ax = plt.subplots(1, figsize=(20, 12))\n",
    "gdf.assign(cl_pr=pr.yb).plot(column='cl_pr', categorical=True, k=5, cmap='OrRd', \n",
    "                                      linewidth=0.1, ax=ax, edgecolor='white', legend=True)\n",
    "\n",
    "plt.title('Average Airbnb Listing Price Quintiles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the spatial lag for listing price using queen weight\n",
    "gdf['w_price'] = ps.lib.weights.lag_spatial(w_queen, gdf['Mean Price'])\n",
    "# list out the name, listing price, and spatial lag for Westminster 018A\n",
    "gdf[['Area', 'Mean Price', 'w_price']].loc[[4000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Task 6'>Task 6<a/>\n",
    "To interpret the spatial lag (w_price) result, we can take Westminster 018A as an example. The average pricing in Westminster 018A is about ¬£255, it is surrounded by neighboring lsoas where the average listing price varies dramatically. We can further check its accuracy by querying the spatial weights matrix to find out the neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the queen spatial weight neighbors for Westminster 018A  \n",
    "w_queen[4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the neighbors' price for private rent\n",
    "nei_price = gdf.loc[w_queen[4000], 'Mean Price']\n",
    "nei_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average value for neighboring price\n",
    "nei_price.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some of the techniques we will be seeing below, it makes more sense to operate with the standardized version of a variable, rather than with the raw one. Standardizing means to substract the average value and divide by the standard deviation each observation of the column. \n",
    "\n",
    "Can you work out the standardized value for airbnb listing price below; and further explore the spatial patterns of the standardized values, or $zscore$, we need to create its spatial lag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "gdf['price_std'] = (gdf['Mean Price'] - gdf['Mean Price'].mean()) / gdf['Mean Price'].std()\n",
    "gdf['w_price_std'] = ps.lib.weights.lag_spatial(w_queen, gdf['price_std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='Spatial Autocorrelation'>Spatial Autocorrelation<a/>\n",
    "\n",
    "Do you still remember `CSR`, the spatial randomness we had on Week 4? It justifies that a spatially random variable follows no discrenible distribution pattern over space, so the variable of interest in a given location will give no information about its value. In another word, if we take our **Airbnb listing price Quintiles** plot as an example, there should be no visible clustering of similar values on the map. However, we can easily spot out the gradient colors change from centre to inner then to outer London, which indicating that it is not CSR. \n",
    "\n",
    "On the contrary, **spatial autocorrelation** could be defined as \"absence of spatial randomness\" in that, for a given dataset, the $similarity$ $in$ $values$ among observations relates to their $locational$ $similarity$; hence relates the target observation's value with values in neighboring locations for specific variable. So in the following, we are trying to generate the meansures for spatial similarity and attribute similarity respectively, which had been utilized widely to generate combined measures for spatial autocorrelation. \n",
    "\n",
    "### <a id='Spatial Similarity'>Spatial Similarity<a/>\n",
    "Spatial weights are used to measure spatial similarity as we've done in previous sections, here we will only use queen contiguity as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_queen = ps.lib.weights.Queen.from_shapefile('data/lsoa_IMD_airbnb_housing.shp')\n",
    "W_queen.transform = 'r' # row-standardize the contiguity weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatial lag has been defined as derived variable pair the attribute similarity up with the spatial similarity.\n",
    "For LSOA $i$ the spatial lag is defined as: ${Mean Price}$$Lag_i$=$‚àë_jw_{i,j}$${Mean Price_j}$, where $j$ are the neighboring lsoas for lsoa $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_Lag = ps.lib.weights.lag_spatial(W_queen, gdf['Mean Price']) #spatial lag of the variable\n",
    "price_LagQ5 = ps.viz.mapclassify.Quantiles(price_Lag, k=5) # let's say k=5 for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 8))\n",
    "gdf.assign(cl_lag=price_LagQ5.yb).plot(column='cl_lag', categorical=True, k=5, cmap='coolwarm', linewidth=0.1, ax=ax, edgecolor='white', legend=True)\n",
    "plt.title('Airbnb Mean Price Lag Quintiles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any differences? Is the quintiles map for spatial lag showing the enhancement of attribute similarity spatially? YES! However, if we want to give any statement on relationship between value of mean airbnb listing price in a lsoa and the value of spatial lag of the price for it, we still need one more step to justify it through statistical measures of spatial autocorrelation. We could use Moran Scatterplot for prelimenary visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Moran Plot'>Moran Plot<a/>\n",
    "\n",
    "Moran scatter plot is similar to normal scatter plot, but widely used to visualize spatial autocorrelation, with the variable of interest against x axis, whilst its spatial lag against y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price=gdf['Mean Price']\n",
    "b,a = np.polyfit(price, price_Lag, 1)\n",
    "f, ax = plt.subplots(1, figsize=(10, 8))\n",
    "plt.plot(price, price_Lag, '.', color='firebrick')\n",
    "\n",
    " # dashed vert at mean of the last year's private rent level\n",
    "plt.vlines(price.mean(), price_Lag.min(), price_Lag.max(), linestyle='--')\n",
    " # dashed horizontal at mean of lagged private rent\n",
    "plt.hlines(price_Lag.mean(), price.min(), price.max(), linestyle='--')\n",
    "\n",
    "# red line of best fit using global I as slope\n",
    "plt.plot(price, a + b*price, 'r')\n",
    "plt.title('Moran Scatterplot')\n",
    "plt.ylabel('Spatial Lag of Airbnb Listing Price')\n",
    "plt.xlabel('Airbnb Mean Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use `seaborn`'s regplot function to get the standardized value for variable(s) of interest plotted, as well as against its spatial lag. So now we will use the standardized values generated in Spatial Lag section for plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(10, 8))\n",
    "# Plot values\n",
    "sns.regplot(x='price_std', y='w_price_std', data=gdf, ci=None)\n",
    "# Add vertical and horizontal lines\n",
    "plt.axvline(0, c='r', alpha=0.5)\n",
    "plt.axhline(0, c='r', alpha=0.5)\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above displays the relationship between the standardized airbnb listing price (`price_std`) and its spatial lag (`w_price_std`) in neighboring lsoas. The linear fit line is the best linear fit to the scatter plot representing the relationship between the two variables.\n",
    "\n",
    "Be obvious from the plot that, these two variables have positive relationship, which leads to next section about two main types of spatial autocorrelations (SA): \n",
    "- **Positive spatial autocorrelation**: similar values tend to group together in similar locations. Generally, high values tend to be surrounded by high values, and low values to be close to low values, with justification of main pattern as clustered.\n",
    "- **Negative spatial autocorrelation**: similar values tend to be dispersed and further apart from each other. Generally, high values tend to be surrounded by low values, and low values to be close to high values, with justification of main pattern as sparsed.\n",
    "\n",
    "Meanwhile, we normally have two main classes of SA: (1) **Global spatial autocorrelation** and (2) **Local spatial autocorrelation**; and use Exploratory Spatial Data Analysis (`ESDA`) tools to realize the analysis purpose, i.e. spatial queries, statistical inference, choropleths, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Global spatial autocorrelation'>Global spatial autocorrelation<a/>\n",
    "Global spatial autocorrelation considers the overall geographical pattern of the target values presented, measure the trend statistically through statements about the degree of clustering, further summarize the result numerically for further visualization. This tool helps to answer questions concerning about geographical distribution patterns of values, the higher adjacency of similar values, etc. We will start interpreting the rationale of global spatial autocorrelation from binary view, and further practice with Moran's I statistic.\n",
    "\n",
    "We can classify the \"low listing price\" and \"high listing price\" dividing by its median value to convert it into binary case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['Mean Price'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = gdf['Mean Price']> gdf['Mean Price'].median()\n",
    "sum(binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among over 4000 lsoas, 2100 of them with average price above the median (¬£73) and remaining below the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Low Price', 'High Price']\n",
    "binary = [labels[i] for i in 1*binary] \n",
    "gdf['binary'] = binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "ax = plt.gca()\n",
    "gdf.plot(column='binary', cmap='binary', edgecolor='grey', legend=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has the plot recalled the image covered in lecture about calculating the joins in neighbors, especially counting the joins by three different types?\n",
    "- BB (Black-Black)\n",
    "- WW (White-White)\n",
    "- BW (Black-White, or White-Black) \n",
    "\n",
    "The joins are reflected in our binary spatial weights object W_queen. So given about half of lsoas are black polygons from this case, how many BB join should we expect for, if they were randomly assigned? We can work out the logic for join counts statistic as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysal.explore import esda \n",
    "binary = 1 * (gdf['Mean Price']> gdf['Mean Price'].median()) # convert back to binary\n",
    "W_queen = lps.weights.Queen.from_dataframe(gdf)\n",
    "W_queen.transform = 'b'\n",
    "np.random.seed(12345)\n",
    "jc = esda.join_counts.Join_Counts(binary, W_queen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Task 7'>Task 7<a/>\n",
    "You may feel free to explore the number of bb, ww and bw by calling `jc.bb`, `jc.ww` and `jc.bw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.mean_bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.ww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(jc.sim_bb, shade=True)\n",
    "plt.vlines(jc.bb, 0, 1, color='r')\n",
    "plt.vlines(jc.mean_bb, 0,1)\n",
    "plt.xlabel('BB Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the black vertical line indicating mean BB count from the synthetic realizations, this density plot shows us the distribution of BB counts, whilst the red line is our observed count, which is extremely higher than the mean value. So let's further check the pseudo p-value for this statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc.p_sim_bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what will you conclude from the value? Write it down below:\n",
    "\n",
    "** ------------------------------------------ ** \n",
    "\n",
    "Since this is below conventional significance levels, we would reject the null of complete spatial randomness in favor of spatial autocorrelation in airbnb listing price in London. \n",
    "\n",
    "To summarize, we created the binary variable for airbnb listing price in London, and explore the join count analysis whilst disregarding information in the original values. So now let's turn back to our real data and test for the spatial autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = esda.moran.Moran(gdf['Mean Price'], W_queen) # call moran function\n",
    "mi.I # print out the moran's I value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Task 8'>Task 8<a/>\n",
    "Plot the statistic against a reference distribution under the null of CSR. \n",
    "    \n",
    "**Hint**: similar to what we did in join count analysis. Call `seaborn`'s kdeplot function. But the expected value should be EI for $mi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(mi.sim, shade=True)\n",
    "plt.vlines(mi.I, 0, 40, color='r')\n",
    "plt.vlines(mi.EI, 0, 40)\n",
    "plt.xlabel(\"Moran's I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the statistical significance\n",
    "mi.p_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just 0.1% (or you may get 0.002, 0.003..., slightly different everytime) and would be considered statistically significant. It means if we try to allocate the same values randomlly over space to get new map, then the Moran's $I$ statistic for new map could have 0.1% possibility to display a larger $I$ value than the one from our real data; while 99.9% of random mapping would receive a smaller $I$ value. As $I$ value could also be interpreted as the slope for Moran plot, the airbnb listing price in London is more concentrated than if it follows a CSR process, hence statistically significance, and has its spatial structure. \n",
    "\n",
    "Besides of calling `esda` in PySAL, we can also realize the Moran's I statistic by directly calling the specific function in `PySAL.explore.esda.Moran`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_price = ps.explore.esda.Moran(gdf['Mean Price'].values, w_queen)  # Moran's I\n",
    "I_price.I, I_price.p_sim  #value of statistic, inference on Moran's I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the $I$ statistic is $0.3478$ for this data, and has a very small $p$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b # I is same as the slope of the line in the scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_price.sim[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the distribution using KDEplot again, with a rug showing all of the simulated points, and a vertical line denoting the observed value of the statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(I_price.sim, shade=True)\n",
    "plt.vlines(I_price.sim, 0, 40)\n",
    "plt.vlines(I_price.I, 0, 40, 'r')\n",
    "plt.xlim([-0.1, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if our $I$ statistic were close to our expected value, I_price.EI , our plot might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(I_price.sim, shade=True)\n",
    "plt.vlines(I_price.sim, 0, 10)\n",
    "plt.vlines(I_price.EI+.01, 0, 40, 'r')\n",
    "plt.xlim([-0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can arrive at the conclusion now: the pattern for airbnb listing price is not spatially random, but instead has signficant spatial association."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Local spatial autocorrelation'>Local spatial autocorrelation<a/>\n",
    "We implement Local Indicators of Spatial Association (LISAs) for Moran‚Äôs I and Getis and Ord‚Äôs G in PySal to detect hotspots.\n",
    "\n",
    "We use Local Moran's I index to test the spatial autocorrelationality. It will measure the spatial autocorrelation in an attribute y measured over n spatial units. To calculate Moran‚Äôs I we first need to create and read in a GAL file for a weights matrix W. In order to get W, we need to work out what polygons neighbour each other (e.g. Queen Style Contiguity Neighbours, and Rook's Case Neighbours, etc.). \n",
    "\n",
    "Read more in R.Bivand (2017) \"Finding Neighbours\".\n",
    "\n",
    "Normally, we use Rook and Queen contiguity weight matrix. Rook weights consider observations as neighboring only when they share an edge; while queen contigutiy weight reflects adjacency relationships whether or not a polygon shares an edge or a vertex with another polygon. They may be different, depending on how the observation and its nearby polygons are configured.\n",
    "\n",
    "Instead of a single $I$ statistic, we have an *array* of local $I_i$ statistics, stored in the `.Is` attribute, and p-values from the simulation are in `p_sim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lisa = ps.explore.esda.Moran_Local(gdf['Mean Price'].values, w_queen, permutations=999)\n",
    "lisa.Is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lisa.q   # quantile classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lisa.p_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lisa.p_sim < 0.05).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Moran scatterplot with statistically significant LISA values highlighted. We want to plot the statistically-significant LISA values in a different color than the others. To do this, first find all of the statistically significant LISAs. Since the $p$-values are in the same order as the $I_i$ statistics, we can do this in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['lag_price'] = ps.lib.weights.lag_spatial(w_queen, gdf['Mean Price'])\n",
    "sigs = gdf['Mean Price'][lisa.p_sim <= .05]\n",
    "W_sigs = gdf['lag_price'][lisa.p_sim <= .05]\n",
    "insigs = gdf['Mean Price'][lisa.p_sim > .05]\n",
    "W_insigs = gdf['lag_price'][lisa.p_sim > .05]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, since we have a lot of points, we can plot the points with a statistically insignficant LISA value lighter using the alpha keyword. In addition, we would like to plot the statistically significant points in a dark red color with triangle shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b,a = np.polyfit(gdf['Mean Price'], gdf['lag_price'], 1)\n",
    "moran=ps.explore.esda.Moran(gdf['Mean Price'].values, w_queen)\n",
    "\n",
    "fig, ax=plt.subplots(1, figsize=(14,10))\n",
    "plt.plot(sigs, W_sigs, '^', color='firebrick')\n",
    "plt.plot(insigs, W_insigs, '.k', alpha=.2)\n",
    " # dashed vert at mean of the last year's PCI\n",
    "plt.vlines(gdf['Mean Price'].mean(), gdf['lag_price'].min(), gdf['lag_price'].max(), linestyle='--')\n",
    " # dashed horizontal at mean of lagged PCI\n",
    "plt.hlines(gdf['lag_price'].mean(), gdf['Mean Price'].min(), gdf['Mean Price'].max(), linestyle='--')\n",
    "\n",
    "# red line of best fit using global I as slope\n",
    "plt.plot(gdf['Mean Price'], a + b*gdf['Mean Price'], 'r')\n",
    "plt.text(s='$I = %.3f$' % moran.I, x=1400, y=500, fontsize=14)\n",
    "plt.text(600, 500, \"HH\", fontsize=15, color='r')\n",
    "plt.text(600, 0, \"HL\", fontsize=15, color='r')\n",
    "plt.text(50, 500, \"LH\", fontsize=15, color='r')\n",
    "plt.text(50, 0, \"LL\", fontsize=15, color='r')\n",
    "plt.title('Moran Scatterplot')\n",
    "plt.ylabel('Spatial Lag of Price')\n",
    "plt.xlabel('Airbnb Listing Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After measuring both global and local spatial autocorrelation, let's visualize the results on London map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysal.viz.splot.esda import lisa_cluster\n",
    "fig, ax=plt.subplots(1, figsize=(14,10))\n",
    "fig = lisa_cluster(lisa, gdf, ax=ax)\n",
    "plt.title(\"LISA Cluster Map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, only high value surrounded by high values has been highlighted. However, we can distinguish the specific type of local spatial association reflected in the four quadrants of the Moran Scatterplot as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = lisa.p_sim < 0.05\n",
    "hotspot = sig * lisa.q==1\n",
    "coldspot = sig * lisa.q==3\n",
    "doughnut = sig * lisa.q==2\n",
    "diamond = sig * lisa.q==4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the boroughs which are hotspots\n",
    "gdf['Mean Price'][hotspot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spots = ['n.sig.', 'hot spot']\n",
    "labels = [spots[i] for i in hotspot*1]\n",
    "\n",
    "from matplotlib import colors\n",
    "hmap = colors.ListedColormap(['red', 'lightgrey'])\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "gdf.assign(cl=labels).plot(column='cl', categorical=True, \\\n",
    "        k=2, cmap=hmap, linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='white', legend=True)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Task 9'>Task 9<a/>\n",
    "Get the information for coldspot, doughnut and diamond by yourself, and plot them out respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spots = ['n.sig.', 'cold spot']\n",
    "labels = [spots[i] for i in coldspot*1]\n",
    "\n",
    "from matplotlib import colors\n",
    "hmap = colors.ListedColormap(['blue', 'lightgrey'])\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "gdf.assign(cl=labels).plot(column='cl', categorical=True, \\\n",
    "        k=2, cmap=hmap, linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='white', legend=True)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spots = ['n.sig.', 'doughnut']\n",
    "labels = [spots[i] for i in doughnut*1]\n",
    "\n",
    "from matplotlib import colors\n",
    "hmap = colors.ListedColormap(['lightblue', 'lightgrey'])\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "gdf.assign(cl=labels).plot(column='cl', categorical=True, \\\n",
    "        k=2, cmap=hmap, linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='white', legend=True)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spots = ['n.sig.', 'diamond']\n",
    "labels = [spots[i] for i in diamond*1]\n",
    "\n",
    "from matplotlib import colors\n",
    "hmap = colors.ListedColormap(['orange', 'lightgrey'])\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "gdf.assign(cl=labels).plot(column='cl', categorical=True, \\\n",
    "        k=2, cmap=hmap, linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='white', legend=True)\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With LISAs in `PySAL`, we classify the observations into 4 groups by its value and the neighbors', exploring their concentration pattern, identifying cases either more similar (**HH, LL**) concentrated or dissimilar (**HL, LH**) around.  The mechanism is similar to Moran's $I$, but applied in this case to each observation. This tool is widely used in identifying clusters in space, and provide suggestive evidence about the processes that might be at work, e.g.identification of spatial clusters of groups of people, delineation of areas with particularly high/low frequency of certain activity, etc.\n",
    "\n",
    "Now let us pull out areas with statistically significant spatial clustering (at the 5% level):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(12, 8))\n",
    "# Plot building blocks\n",
    "gdf.plot(ax=ax, facecolor='1', linewidth=0.1)\n",
    "# Plot HH clusters\n",
    "hh = gdf.loc[(hotspot) & (sig==True), 'geometry']\n",
    "hh.plot(ax=ax, color='red', linewidth=0.1, edgecolor='w')\n",
    "# Plot LL clusters\n",
    "ll = gdf.loc[(coldspot) & (sig==True), 'geometry']\n",
    "ll.plot(ax=ax, color='blue', linewidth=0.1, edgecolor='w')\n",
    "# Plot LH clusters\n",
    "lh = gdf.loc[(doughnut) & (sig==True), 'geometry']\n",
    "lh.plot(ax=ax, color='#83cef4', linewidth=0.1, edgecolor='w')\n",
    "# Plot HL clusters\n",
    "hl = gdf.loc[(diamond) & (sig==True), 'geometry']\n",
    "hl.plot(ax=ax, color='orange', linewidth=0.1, edgecolor='w')\n",
    "# Non-significant\n",
    "ns = gdf.loc[sig!=True, 'geometry']\n",
    "ns.plot(ax=ax, color='0.75', linewidth=0.1, edgecolor='w')\n",
    "# Style and draw\n",
    "f.suptitle('LISA for Airbnb Listing Price', size=20)\n",
    "f.set_facecolor('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='Spatial Regression'>Spatial Regression</a>\n",
    "We need to go back to last week's OLS regression configuration first, and use Moran's I tool to check for spatial autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysal.model.spreg import ols\n",
    "from pysal.model.spreg import ml_error\n",
    "from pysal.model.spreg import ml_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the .dbf file from your shapefile data\n",
    "f = ps.lib.io.open('data/lsoa_IMD_airbnb_housing.dbf','r')\n",
    "# Read in the listing_price (dependent variable) into an array y\n",
    "y = np.array(f.by_col['Mean Price'])\n",
    "y.shape = (len(y),1)\n",
    "# value for independent variables into a one dimmensional array X. \n",
    "# You can feel free to change the independant variables\n",
    "X= []\n",
    "X.append(f.by_col['Value']) # average house price\n",
    "X.append(f.by_col['IncScore']) # Income score in 2019\n",
    "X.append(f.by_col['EduScore']) # Education score in 2019\n",
    "X.append(f.by_col['BHSScore']) # Barrier to Housing Services score in 2019\n",
    "X.append(f.by_col['IMDScore']) # Deprivation index in 2019\n",
    "X.append(f.by_col['Property C']) # number of airbnbs\n",
    "X = np.array(X).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = ps.explore.esda.moran.Moran(gdf['Mean Price'], w_queen, two_tailed=False)\n",
    "print(\"The Statistic Moran's I is: \"+str(\"%.4f\"%mi.I),\n",
    "      \"\\nThe Expected Value for Statistic I is: \"+str(\"%.4f\"%mi.EI),\n",
    "      \"\\nThe Significance Test Value is: \"+str(\"%.4f\"%mi.p_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Spatial Lag model'>Spatial Lag model</a>\n",
    "In a similar way to how we have included the spatial lag, one could think the airbnb listing prices surrounding a given property also enter its own price function. Recall your memory on Spatial Lag model from lecture, and use `ML_Lag` class in `pysal.model.spreg` to estimate this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "REGRESSION\n----------\nSUMMARY OF OUTPUT: MAXIMUM LIKELIHOOD SPATIAL LAG (METHOD = FULL)\n-----------------------------------------------------------------\nData set            :lsoa_airbnb_housing\nWeights matrix      :     w_queen\nDependent Variable  :Airbnb_price                Number of Observations:        4201\nMean dependent var  :     87.6554                Number of Variables   :           8\nS.D. dependent var  :     71.2105                Degrees of Freedom    :        4193\nPseudo R-squared    :      0.3039\nSpatial Pseudo R-squared:  0.2929\nSigma-square ML     :    3529.299                Log likelihood        :  -23131.647\nS.E of regression   :      59.408                Akaike info criterion :   46279.294\n                                                 Schwarz criterion     :   46330.039\n\n------------------------------------------------------------------------------------\n            Variable     Coefficient       Std.Error     z-Statistic     Probability\n------------------------------------------------------------------------------------\n            CONSTANT      43.3968037       4.1305919      10.5061948       0.0000000\n         house_price       0.0000514       0.0000025      20.7273640       0.0000000\n        income_score    -111.5059253      44.9099838      -2.4828761       0.0130326\n     education_score       0.0110109       0.1571298       0.0700751       0.9441339\n       barrier_score      -0.3036298       0.1390804      -2.1831237       0.0290267\n                 IMD       0.6799258       0.3628787       1.8736997       0.0609718\n          Num_Airbnb       0.3584672       0.0393066       9.1197644       0.0000000\n      W_Airbnb_price       0.0319444       0.0033763       9.4613914       0.0000000\n------------------------------------------------------------------------------------\n================================ END OF REPORT =====================================\n"
    }
   ],
   "source": [
    "import pysal.model.spreg as psms\n",
    "\n",
    "spat_lag = psms.ML_Lag(y,X,w_queen, name_y='Airbnb_price', \n",
    "                       name_x=['house_price', 'income_score','education_score', 'barrier_score', 'IMD', 'Num_Airbnb'], \n",
    "                       name_w='w_queen', name_ds='lsoa_airbnb_housing')\n",
    "print(spat_lag.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:.6f}\".format(spat_lag.rho)) #estimate of spatial autoregressive coefficient\n",
    "print(np.around(spat_lag.betas, decimals=4)) #array of estimated coefficients\n",
    "print(\"{0:.6f}\".format(spat_lag.mean_y)) #Mean of dependent variable\n",
    "print(\"{0:.6f}\".format(spat_lag.std_y))#Standard deviation of dependent variable\n",
    "print(np.around(np.diag(spat_lag.vm1), decimals=4))#Variance covariance matrix (k+2 x k+2) includes sigma2\n",
    "print(np.around(np.diag(spat_lag.vm), decimals=4)) #Variance covariance matrix (k+1 x k+1) - includes lambda\n",
    "print(\"{0:.6f}\".format(spat_lag.sig2))#Sigma squared used in computations\n",
    "print(\"{0:.6f}\".format(spat_lag.logll)) #maximized log-likelihood (including constant terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, results are again very similar in all the other variable. It is also very clear that the estimate of the spatial lag of price is statistically significant. This points to evidence that there are processes of spatial interaction between property owners when they set their price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Spatial Error model'>Spatial Error model</a>\n",
    "\n",
    "Now, we can use a spatial error model to account for spatial non-independence. The **spreg** module in PySAL has several different functions for creating a spatial regression model. \n",
    "\n",
    "ReferenceÔºö\n",
    "[1] Anselin, L. (1988) \"Spatial Econometrics: Methods and Models\".\n",
    "    Kluwer Academic Publishers. Dordrecht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spat_err = psms.ML_Error(y,X,w_queen, name_y='Airbnb_price', \n",
    "                       name_x=['house_price', 'income_score','education_score', 'barrier_score', 'IMD', 'Num_Airbnb'], \n",
    "                       name_w='w_queen', name_ds='lsoa_airbnb_housing')\n",
    "print(spat_err.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:.6f}\".format(spat_err.lam)) #estimate of spatial autoregressive coefficient\n",
    "print(np.around(spat_err.betas, decimals=4)) #array of estimated coefficients\n",
    "print(\"{0:.6f}\".format(spat_err.mean_y)) #Mean of dependent variable\n",
    "print(\"{0:.6f}\".format(spat_err.std_y))#Standard deviation of dependent variable\n",
    "print(np.diag(spat_err.vm)) #Variance covariance matrix (k+1 x k+1) - includes lambda\n",
    "print(\"{0:.6f}\".format(spat_err.sig2[0][0]))#Sigma squared used in computations\n",
    "print(\"{0:.6f}\".format(spat_err.logll)) #maximized log-likelihood (including constant terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Task 10'>Task 10</a>\n",
    "Discuss you interpretation on the results from Spatial Lag Model and Spatial Error Model with your neighbors, comparing them with the OLS regression result last week, and summarize your conclusions below:\n",
    "\n",
    "**----------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Prediction performance of spatial models'>Prediction performance of spatial models</a>\n",
    "We can use the mean squared error (MSE), a standard metric of accuracy in the machine learning literature, to evaluate whether explicitly spatial models are better than traditional, non-spatial ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = psms.OLS(y, X, name_y='Airbnb_price', \n",
    "              name_x=['house_price', 'income_score','education_score', 'barrier_score', 'IMD', 'Num_Airbnb'], \n",
    "              name_ds='lsoa_airbnb_housing')\n",
    "sl = psms.ML_Lag(y, X, w_queen, name_y='Airbnb_price', \n",
    "                       name_x=['house_price', 'income_score','education_score', 'barrier_score', 'IMD', 'Num_Airbnb'], \n",
    "                       name_w='w_queen', name_ds='lsoa_airbnb_housing')\n",
    "se = psms.ML_Error(y, X, w_queen, name_y='Airbnb_price', \n",
    "                       name_x=['house_price', 'income_score','education_score', 'barrier_score', 'IMD', 'Num_Airbnb'], \n",
    "                       name_w='w_queen', name_ds='lsoa_airbnb_housing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SL     3529.299498\nOLS    3624.876483\nSE     3630.180809\ndtype: float64"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "mses = pd.Series({'OLS': mse(y, m1.predy.flatten()), \n",
    "                  'SL': mse(y, sl.predy.flatten()), \n",
    "                  'SE': mse(y, se.predy.flatten())\n",
    "                    })\n",
    "mses.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inclusion of the spatial lag of price marginally reduces the MSE, however, does a better job at improving the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='GWR Prediction'>GWR Prediction</a>\n",
    "Geographically weighted regression (**GWR**) can fit Gaussian, Poisson, and logistic models to estimate a 'GWR Results' object. Now let's compare the maps $before$ and $after$ applying the GWR prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = np.min(gdf['Mean Price']), np.max(gdf['Mean Price']) \n",
    "ax = gdf.plot('Mean Price', vmin=vmin, vmax=vmax, figsize=(8,8), cmap='Reds')\n",
    "ax.set_title('Mean Price'+' t-vals')\n",
    "fig = ax.get_figure()\n",
    "cax = fig.add_axes([1.0, 0.3, 0.02, 0.4]) # the position and size of colormap legend bar\n",
    "sm_price = plt.cm.ScalarMappable(norm=plt.Normalize(vmin=vmin, vmax=vmax), cmap='Reds')\n",
    "sm_price._A = []\n",
    "fig.colorbar(sm_price, cax=cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data into design matrix and coordinates\n",
    "\n",
    "# Dependent variable\n",
    "y = gdf['Mean Price']\n",
    "y = np.array(y).reshape(-1,1) # make array change the list format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Design matrix - covariates - intercept added automatically\n",
    "house_price = np.array(gdf.Value).reshape(-1,1)\n",
    "income_score = np.array(gdf.IncScore).reshape(-1,1)\n",
    "educa_score = np.array(gdf.EduScore).reshape(-1,1)\n",
    "barri_score = np.array(gdf.BHSScore).reshape(-1,1)\n",
    "imd_score = np.array(gdf.IMDScore).reshape(-1,1)\n",
    "num_airbnb = np.array(gdf['Property C']).reshape(-1,1)\n",
    "\n",
    "X = np.hstack([house_price, income_score, educa_score, barri_score, imd_score, num_airbnb])\n",
    "labels = ['Intercept', 'house_price', 'income_score', 'educa_score', 'barri_score', 'imd_score', 'num_airbnb']\n",
    "\n",
    "# standardization\n",
    "X_s = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "y_s = (y - y.mean(axis=0)) / y.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coordinates for calibration points\n",
    "def getXY(pt):\n",
    "    return (pt.x, pt.y)\n",
    "centroidseries = gdf['geometry'].centroid\n",
    "u,v = [list(t) for t in zip(*map(getXY, centroidseries))]\n",
    "\n",
    "coords = list(zip(u,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare dataset inputs\n",
    "g_y = gdf['Mean Price'].values.reshape((-1,1))\n",
    "g_X = gdf[['Value', 'IncScore','EduScore', 'BHSScore', 'IMDScore', 'Property C']].values\n",
    "\n",
    "g_coords = list(zip(u,v))\n",
    "\n",
    "# Standardised our data to have mean of 0 and standard deviation of 1\n",
    "g_X = (g_X - g_X.mean(axis=0)) / g_X.std(axis=0)\n",
    "\n",
    "g_y = g_y.reshape((-1,1))\n",
    "\n",
    "g_y = (g_y - g_y.mean(axis=0)) / g_y.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1295.0"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "from pysal.model.mgwr.sel_bw import Sel_BW\n",
    "\n",
    "# Select bandwidth for kernel\n",
    "bw = Sel_BW(g_coords, \n",
    "            g_y, # Independent variable\n",
    "            g_X, # Dependent variable\n",
    "            fixed=False, # True for fixed bandwidth and false for adaptive bandwidth\n",
    "            spherical=True) # Spherical coordinates (long-lat) or projected coordinates\n",
    "# calculate the optimum bandwidth for our local regression\n",
    "bw.search(bw_min=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysal.model.mgwr.gwr import GWR\n",
    "#Instantiate GWR model and then estimate parameters and diagnostics using fit method\n",
    "model = GWR(coords, y, X, bw.bw[0])\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results in a set of mappable results \n",
    "results.params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (results.bse[0:10, 1])\n",
    "print (results.tvalues[0:10, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map Parameter estimates and T-vals for each covariate\n",
    "for param in range(results.params.shape[1]):\n",
    "    gdf[str(param)] = results.params[:,param]\n",
    "    vmin, vmax = np.min(gdf[str(param)]), np.max(gdf[str(param)]) \n",
    "    ax = gdf.plot(str(param), vmin=vmin, vmax=vmax, figsize=(8,8), cmap='YlOrRd')\n",
    "    ax.set_title(labels[param] + ' Estimates')\n",
    "    fig = ax.get_figure()\n",
    "    cax = fig.add_axes([1.0, 0.3, 0.03, 0.4])\n",
    "    sm = plt.cm.ScalarMappable(norm=plt.Normalize(vmin=vmin, vmax=vmax), cmap='YlOrRd')\n",
    "    sm._A = []\n",
    "    fig.colorbar(sm, cax=cax)\n",
    "    \n",
    "    gdf[str(param)] = results.tvalues[:,param]\n",
    "    vmin, vmax = np.min(gdf[str(param)]), np.max(gdf[str(param)]) \n",
    "    ax = gdf.plot(str(param), vmin=vmin, vmax=vmax, figsize=(8,8), cmap='Greys')\n",
    "    ax.set_title(labels[param] + ' t-vals')\n",
    "    fig = ax.get_figure()\n",
    "    cax = fig.add_axes([1.0, 0.3, 0.02, 0.4])\n",
    "    sm = plt.cm.ScalarMappable(norm=plt.Normalize(vmin=vmin, vmax=vmax), cmap='Greys')\n",
    "    sm._A = []\n",
    "    fig.colorbar(sm, cax=cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(results.localR2))\n",
    "print (np.mean(results.localR2))\n",
    "print (results.localR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map local R-square values which is a weighted R-square at each observation location\n",
    "\n",
    "gdf['localR2'] = results.localR2\n",
    "vmin, vmax = np.min(gdf['localR2']), np.max(gdf['localR2']) \n",
    "ax = gdf.plot('localR2', vmin=vmin, vmax=vmax, figsize=(8,8), cmap='PuBuGn')\n",
    "ax.set_title('Local R-Squared')\n",
    "fig = ax.get_figure()\n",
    "cax = fig.add_axes([1.0, 0.3, 0.02, 0.4])\n",
    "sm = plt.cm.ScalarMappable(norm=plt.Normalize(vmin=vmin, vmax=vmax), cmap='PuBuGn')\n",
    "sm._A = []\n",
    "fig.colorbar(sm, cax=cax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits!\n",
    "\n",
    "#### Contributors:\n",
    "The following individual(s) have contributed to these teaching materials: Yijing Li (yijing.li@kcl.ac.uk).\n",
    "\n",
    "#### License\n",
    "These teaching materials are licensed under a mix of [The MIT License](https://opensource.org/licenses/mit-license.php) and the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 license](https://creativecommons.org/licenses/by-nc-sa/4.0/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit8f5d7887469441e1a994bf5b866609b0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}